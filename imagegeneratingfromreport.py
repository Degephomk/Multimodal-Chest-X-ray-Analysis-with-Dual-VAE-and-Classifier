# -*- coding: utf-8 -*-
"""deeplearniningcode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F-bgHORzGxUitOOQxEvrW2lw8q9HzX-y
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

"""**Import modules**"""

from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from transformers import GPT2TokenizerFast, GPT2LMHeadModel
import pandas as pd
from sklearn.model_selection import train_test_split
from PIL import Image
import torch.nn.functional as F
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import pytorch_lightning as pl
from torchmetrics import Accuracy
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import cv2
import os

# create a directory for our model
save_directory = "./saved_model"
os.makedirs(save_directory, exist_ok=True)

"""# Data Understanding

In this section, we will examine the dataset containing chest X-ray images and their associated metadata files. The images are stored in a separate directory called the `images` folder. Alongside the images, there are two CSV files: `projection.csv` and `reports.csv`.

## Dataset Overview

### `projection.csv`
This file contains metadata for each image. It provides information about the projection type (either `frontal` or `lateral`) associated with each image filename. Each entry links a specific image to its projection category.

### `reports.csv`
This file includes detailed metadata about the X-ray images. It provides descriptions such as findings, problems, and impressions for each image. This data is critical for understanding the clinical context of the X-rays and for supporting tasks like diagnosis, classification, or research.

---

## Goals of this Section

1. **Understand the structure of the dataset**:  
   We will explore the folder and files, ensuring we have a clear grasp of how the images and metadata are organized.

2. **Analyze the CSV files**:  
   By inspecting the columns and content of both `projection.csv` and `reports.csv`, we aim to extract meaningful information for subsequent processing and analysis.

3. **Correlate metadata with images**:  
   We will link the metadata provided in the CSV files to their corresponding images in the `images` folder, ensuring consistency and completeness.

---



"""

# Define the paths to the dataset files and images
projections_path = '/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv'
reports_path = '/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv'
img_dir = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'

"""## Metadata Analysis Report

Analysing the report file for the chest X-ray dataset, we identified 8 columns and a total of 3851 records.

| Column Name           | Description                                                                
|-------------------	|---------------------------------------------------------------------------------------------|
| uid            	    |  A unique identifier for each row in the report dataset.                  
| MeSH                	| Medical Subject Headings, which are standardized terms used to describe medical topics or conditions associated with the report.                                                  	|  
| Problems        	    | Specifies the medical problems or findings observed in the X-ray or related to the patient.                                             |
| image         	    |   Describes the type and view of the imaging study performed..         	                                                     |   
| indication           	|  The clinical reason for performing the imaging study.|   
| comparison	        |   Notes whether the radiologist compared the current imaging with any previous imaging studies.        	|   
| findings          	|     Contains the detailed observations made by the radiologist based on the imaging.                                                |
| impression          	|     Summarizes the radiologist’s overall interpretation or diagnosis based on the imaging findings.           	|
"""

reports_data=pd.read_csv(reports_path)
# What are the datatypes of the columns
reports_data.info()
display(reports_data.head(5))
# total number of elements in the dataset
reports_data.size
# value distribution
display(reports_data.value_counts(['MeSH']))
display(reports_data.value_counts(['Problems']))
display(reports_data.value_counts(['image']))

"""# Images


"""

print(reports_data.columns.tolist())
#Missing values
missing_value_counts = reports_data.isna().sum()
display(missing_value_counts)

projections_data=pd.read_csv(projections_path)
display(reports_data.head(5))
# Size of the report data
display(reports_data.shape)
#size of the projections data
display(projections_data.shape)

"""*******

## Images
"""

# List all .png files in the images folder
images_data = [f for f in os.listdir(img_dir) if f.endswith('.png')]
total_images = len(images_data)
# total image files
display(total_images)
# Create a Pandas DataFrame with the image file paths
images_data_df = pd.DataFrame({'image_path': [os.path.join(img_dir, f) for f in images_data]})
display(images_data_df.head(5))
# Display only the first 5 images
def display_first_five_images(images_data_df):
    for index, row in images_data_df.head(5).iterrows():  # Use .head(5) to get the first 5 rows
        # Load the image
        img = cv2.imread(row['image_path'])
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for correct color display

        # Display the image
        plt.figure(figsize=(10, 10))
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"Image: {os.path.basename(row['image_path'])}")
        plt.show()

# Display the first 5 images
display_first_five_images(images_data_df)

"""## Image projections Analysis

"""

projections_data=pd.read_csv(projections_path)
# What are the datatypes of the columns
projections_data.info()
display(projections_data.head(5))
# total number of elements in the dataset
projections_data.size
# value distribution
display(projections_data.value_counts(['projection']))

def analyze_xray_report_dataset(reports_path, projections_path, img_dir):
    """
    We analyze the chest X-ray report dataset, emphasizing the identified issues and detecting rows with
    formatting irregularities.
    reports_path (str): File path to the CSV containing the chest X-ray reports.
    projections_path (str): File path to the CSV with projection information.
    img_dir (str): Directory where the corresponding images are stored.
    """
    print("=== DATASET ANALYSIS ===\n")

    # Analyze the problems
    # Separate multiple problems (delimited by `;`)
    all_problems = []

    # Count rows with malformed problems
    correct_format_problem_count = 0
    incorrect_format_problem_count = 0
    for problems in reports_data['Problems'].dropna():
        problems_str = str(problems)
        # Check for formatting issues
        if problems_str == "No Indexing" or "," in problems_str:
            incorrect_format_problem_count += 1
        else:
            correct_format_problem_count += 1
        all_problems.extend([p.strip() for p in problems_str.split(';')])

    # Count the problems
    problem_counts = pd.Series(all_problems).value_counts()

    print("occurrences of each unique problem:")
    print("------------------------------------")
    for problem, count in problem_counts.items():
        print(f"{problem}: {count} occurrences")

    print(f"\nTotal unique problems: {len(problem_counts)}\n")

    print("Formatting analysis:")
    print("--------------------")
    print(f"Rows with correct formatting: {correct_format_problem_count}")
    print(f"Rows with incorrect formatting: {incorrect_format_problem_count}")

    # Create a figure with 3 subplots
    plt.figure(figsize=(20, 15))

    # 1. Top 15 problems
    plt.subplot(3, 1, 1)
    top_problems = problem_counts.head(15)
    sns.barplot(x=top_problems.values, y=top_problems.index)
    plt.title('Top 15 Most Common Problems in X-rays')
    plt.xlabel('Number of Occurrences')
    plt.ylabel('Problem')

    # 2. Pie chart for formatting quality
    plt.subplot(3, 1, 2)
    labels = ['Correct Formatting', 'Incorrect Formatting']
    sizes = [correct_format_problem_count, incorrect_format_problem_count]
    colors = ['lightgreen', 'lightcoral']
    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
    plt.title('Distribution of Formatting Quality in Problems')


    # 3. Full problem distribution (logarithmic scale)
    plt.subplot(3, 1, 3)

    # Create a line chart for all problems
    x = range(len(problem_counts))
    plt.plot(x, problem_counts.values, 'b-')
    plt.fill_between(x, problem_counts.values, alpha=0.3)
    plt.yscale('log')  # Logarithmic scale for better visualization of differences
    plt.title('Full Problem Distribution (Logarithmic Scale)')
    plt.xlabel('Problem Index')
    plt.ylabel('Number of Occurrences (log)')
    plt.grid(True, which="both", ls="-", alpha=0.2)

    # Add annotations to highlight imbalance
    plt.annotate(f'Most frequent problem: {problem_counts.index[0]}\n({problem_counts.values[0]} occurrences)',
                xy=(0, problem_counts.values[0]),
                xytext=(10, problem_counts.values[0]*1.2),
                arrowprops=dict(facecolor='black', shrink=0.05))

    plt.annotate(f'Least frequent problem: {problem_counts.index[-1]}\n({problem_counts.values[-1]} occurrences)',
                xy=(len(problem_counts)-1, problem_counts.values[-1]),
                xytext=(len(problem_counts)-30, problem_counts.values[-1]*2),
                arrowprops=dict(facecolor='black', shrink=0.05))

    plt.tight_layout()
    plt.show()

    return problem_counts, correct_format_problem_count, incorrect_format_problem_count


# Call the function and analyze the dataset
problems_count, missing_probs, missing_imgs = analyze_xray_report_dataset(
    reports_path,
    projections_path,
    img_dir
)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def plot_problem_distribution(reports_path):
    """
    Plots the distribution of problems in chest X-ray reports.

    Parameters:
    reports_path (str): Path to the CSV file containing the reports.
    """
    # Load and preprocess the data
    reports_df = pd.read_csv(reports_path)

    # Extract and count problems
    all_problems = []
    for problems in reports_df['Problems'].dropna():
        problems_str = str(problems)
        all_problems.extend([p.strip() for p in problems_str.split(';')])

    problem_counts = pd.Series(all_problems).value_counts()

    # Create a figure with two side-by-side subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    fig.suptitle('Distribution of Problems in X-ray Reports', fontsize=16, y=1.05)

    # Plot 1: Top 15 problems (linear scale)
    top_15 = problem_counts.head(15)
    sns.barplot(x=top_15.values, y=top_15.index, ax=ax1, palette='viridis')
    ax1.set_title('Top 15 Most Common Problems', pad=20)
    ax1.set_xlabel('Number of Occurrences')
    ax1.set_ylabel('Problem')

    # Add values on the bars
    for i, v in enumerate(top_15.values):
        ax1.text(v + 1, i, f'{v:,}', va='center')

    # Plot 2: Full distribution (logarithmic scale)
    x = range(len(problem_counts))
    ax2.plot(x, problem_counts.values, 'b-', linewidth=2)
    ax2.fill_between(x, problem_counts.values, alpha=0.3)
    ax2.set_yscale('log')
    ax2.set_title('Complete Distribution (Logarithmic Scale)', pad=20)
    ax2.set_xlabel('Problem Index')
    ax2.set_ylabel('Number of Occurrences (log)')
    ax2.grid(True, which="both", ls="-", alpha=0.2)

    # Add annotations for extreme values
    ax2.annotate(f'Max: {problem_counts.values[0]:,} occurrences\n({problem_counts.index[0]})',
                xy=(0, problem_counts.values[0]),
                xytext=(10, problem_counts.values[0]*1.2),
                arrowprops=dict(facecolor='black', shrink=0.05))

    ax2.annotate(f'Min: {problem_counts.values[-1]:,} occurrences\n({problem_counts.index[-1]})',
                xy=(len(problem_counts)-1, problem_counts.values[-1]),
                xytext=(len(problem_counts)-30, problem_counts.values[-1]*2),
                arrowprops=dict(facecolor='black', shrink=0.05))

    plt.tight_layout()
    plt.show()

    # Print some statistics
    print("\nStatistics on the problem distribution:")
    print(f"Total unique problems: {len(problem_counts):,}")
    print(f"Most frequent problem: {problem_counts.index[0]} ({problem_counts.values[0]:,} occurrences)")
    print(f"Least frequent problem: {problem_counts.index[-1]} ({problem_counts.values[-1]:,} occurrences)")

# Example usage
plot_problem_distribution(reports_path)

import pandas as pd

# Create an empty DataFrame for frontal images, lateral images, captions, and problems
df = pd.DataFrame({'front_img': [], 'lateral_img': [], 'captions': [], 'problems': []})
count=0
rows=0
# Loop through each unique `uid` in the image DataFrame
for uid in projections_data['uid'].unique():
    rows+=1
    # Find all frontal images for the current `uid`
    front_image = projections_data.loc[(projections_data['uid'] == uid) & (projections_data['projection'] == 'Frontal')]

    # Find all lateral images for the current `uid`
    lateral_img = projections_data.loc[(projections_data['uid'] == uid) & (projections_data['projection'] == 'Lateral')]

    # If both frontal and lateral images are available
    if not front_image.empty and not lateral_img.empty:
        count += 1
        # Find the corresponding caption (findings) for the current `uid`
        caption = reports_data.loc[reports_data['uid'] == uid, 'findings'].values[0]
        # Find the corresponding problems for the current `uid`
        problems = reports_data.loc[reports_data['uid'] == uid, 'Problems'].values[0]

        # Add a new row to the DataFrame
        df = pd.concat([df, pd.DataFrame([{
            'uid':uid,
            'front_img': front_image.iloc[0]['filename'],
            'lateral_img': lateral_img.iloc[0]['filename'],
            'captions': caption,
            'problems': problems
        }])], axis=0)

# Update the paths of the images
loc = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'
df['front_img'] = loc + df['front_img']
df['lateral_img'] = loc + df['lateral_img']

# Reset the index to ensure it is unique
# This ensures each row has a unique ID
df = df.reset_index(drop=True)

print(df.columns.tolist())
#Missing values
print(rows)
print(count)
display(df.head(5))

df.shape

# Count the occurrence of reported issues
issue_counts = {}
for issues in df['problems'].dropna():
    for issue in issues.split(';'):
        issue = issue.strip()  # Remove extra spaces
        issue_counts[issue] = issue_counts.get(issue, 0) + 1

display(len(issue_counts))
issues_to_remove = [
    "No Indexing",
    "Technical Quality of Image Unsatisfactory",
    "Foreign Bodies",
    "Calcinosis",
    "Density",
    "Diaphragm",
    "Diaphragmatic Eventration",
    "Hernia, Hiatal",
    "Catheters, Indwelling",
    "Surgical Instruments",
    "Implanted Medical Device",
    "Medical Device",
    "Lucency",
    "Tube, Inserted",
    "Markings",
]

# Total issues to remove
display(len(issues_to_remove))
print(issues_to_remove)

# Function to filter issues in a row
def filter_issues(issues, min_frequency=20):
    """
    Removes issues with occurrences below the minimum frequency.
    Also removes the issue "No Indexing."
    Returns a string with the filtered issues or None if none remain.
    """
    if not issues:  # Check if the field is empty or NaN
        return None
    filtered_issues = [
        issue.strip() for issue in issues.split(';')
        if issue_counts.get(issue.strip(), 0) >= min_frequency and issue.strip() not in issues_to_remove
    ]
    return ';'.join(filtered_issues) if filtered_issues else None

# Apply the filter to the issues
df['problems'] = df['problems'].apply(filter_issues)
display(df.head(5))
display(issue_counts)

# Count of rows with no issues
count_no_issues = df['problems'].isna().sum()
print(count_no_issues)

# Drop rows where 'issues' column has None or NaN values
df = df.dropna(subset=['problems'])
remaining_rows = df.shape[0]  # Get the number of remaining rows
print(remaining_rows)

# Missing Values
missing_data_counts = df.isna().sum()
display(missing_data_counts)

def count_problems_occurrences(df):
    """
    Counts and prints the problems and the number of occurrences of each in the dataset.
    Args:
    df (pd.DataFrame): DataFrame containing the 'problems' column.
    """
    # Check if the 'problems' column exists
    if 'problems' not in df.columns:
        print("The 'problems' column is not present in the DataFrame.")
        return

    # Split multiple problems if they are separated by semicolons
    all_problems = df['problems'].dropna().str.split(';')
    print(len(all_problems))

    # Flatten the list of problems and count occurrences
    problem_counts = {}
    for problem_list in all_problems:
        for problem in problem_list:
            problem = problem.strip()  # Remove any leading or trailing spaces
            problem_counts[problem] = problem_counts.get(problem, 0) + 1

    # Sort problems by number of occurrences (descending order)
    sorted_problems = sorted(problem_counts.items(), key=lambda x: x[1], reverse=True)
     # Print the results
    print("Problems and occurrences:")
    for problem, count in sorted_problems:
        print(f"{problem}: {count}")

# count problem occurrences
count_problems_occurrences(df)
display(df.head(5))

"""We categorizes chest X-ray problems into four  macro categories: "Normal","Pulmonary", "Cardiovascular" and "Musculoskeletal" we assigned  each problem to a category based on predefined lists, labeling ambiguous cases as "Multiple" (later removed) and discarding unmatched entries.

Data cleaning ensures only relevant records remain. The category distribution is visualized using bar and pie charts for insights. One-hot encoding transforms the categories into a binary matrix, which is then converted into a PyTorch tensor. The encoded data was then converted into a PyTorch tensor, making it ready for use deep learnining models.
"""

# Categories
category_groups = {
    "Normal": [
        "normal"
    ],
    "Pulmonary": [
        "Lung",
        "Pulmonary Atelectasis",
        "Pulmonary Congestion",
        "Pulmonary Disease, Chronic Obstructive",
        "Pulmonary Emphysema",
        "Pulmonary Edema",
        "Infiltrate",
        "Emphysema",
        "Granuloma",
        "Airspace Disease",
        "Pleural Effusion",
        "Pneumonia",
        "Thickening",
        "Consolidation",
        "Pneumothorax",
        "Granulomatous Disease",
        "Calcified Granuloma",
        "Costophrenic Angle",
        "Opacity",
        "Nodule"
    ],
    "Cardiovascular": [
        "Cardiomegaly",
        "Aorta",
        "Aorta, Thoracic",
        "Atherosclerosis",
        "Pulmonary Edema",
        "Cardiac Shadow",
        "Mediastinum",
    ],
    "Musculoskeletal": [
        "Thoracic Vertebrae",
        "Spine",
        "Scoliosis",
        "Kyphosis",
        "Deformity",
        "Osteophyte",
        "Spondylosis",
        "Cicatrix",
        "Fractures, Bone",
        "Bone Diseases, Metabolic",
        "Spinal Fusion",
        "Arthritis",
    ]
}

# Function to Assign Categories Based on Problems
def categorize_problems(problems, category_dict):
    """
    Assigns a macro category based on the problems using the category dictionary.
    """
    if not problems:  # If there are no problems, return None
        return None

    assigned_categories = set()  # Track assigned macro categories
    for problem in problems.split(';'):
        problem = problem.strip()
        for category, problem_list in category_dict.items():
            if problem in problem_list:
                assigned_categories.add(category)

    # If no category is assigned, return None
    if len(assigned_categories) == 0:
        return None
    # If multiple categories are assigned, return 'Multiple'
    elif len(assigned_categories) > 1:
        return 'Multiple'
    # Otherwise, return the assigned macro category
    else:
        return assigned_categories.pop()

# Applying the categorization function to the 'problems' column
df['categories'] = df['problems'].apply(lambda x: categorize_problems(x, category_groups))

# Displaying the number of rows after categorization
display(df.shape[0])

# Visualizing the value distribution for the 'category' column
display(df['categories'].value_counts())

# Counting missing values in the dataset
missing_values_count = df.isna().sum()
display(missing_values_count)

# Drop rows with 'Multiple' categories and rows with no assigned category
df = df[df['categories'] != 'Multiple']
display(df.shape[0])

# Count occurrences of each category
category_distribution = df['categories'].value_counts()

# Create a figure with two side-by-side subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
fig.suptitle('Category Distribution in X-ray Reports', fontsize=16, y=1.05)

# Plot 1: Bar Chart for Category Distribution
sns.barplot(x=category_distribution.values, y=category_distribution.index, ax=ax1, palette='viridis')
ax1.set_title('Category Count', pad=20)
ax1.set_xlabel('Number of Occurrences')
ax1.set_ylabel('Category')

# One-hot encode the 'category' column without modifying the original DataFrame
encoder = OneHotEncoder(sparse=False)
one_hot_labels = encoder.fit_transform(df[['categories']])

# Convert the one-hot encoded labels into a PyTorch tensor
labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float)

# Print the tensor to verify
print(labels_tensor)

df = df.dropna(subset=['captions'])
df.shape

!pip install torch torchvision pytorch-lightning timm pandas pillow scikit-learn

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import pandas as pd
import timm
import pytorch_lightning as pl
from torchmetrics import Accuracy
from sklearn.model_selection import train_test_split

"""# Multimodal Learning: Classification, Image Reconstruction, and Synthetic Image Generation

## Environment Setup & Configuration

We implemented a configuration class (Config) to define key hyperparameters and settings for our deep learning model. These parameters ensure consistency across different training runs and improve reproducibility.

1. Configuration Class (Config)
latent_dim = 256:The dimension of the latent space, which determines the size of the feature representations in the model.
batch_size = 32: Specifies the number of images processed in a single batch.
epochs = 30: The number of times the entire dataset is passed through the model during training.
lr = 1e-4: Learning rate, which controls the step size during gradient updates for optimization.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'):  whether to use a GPU (if available) or fallback to the CPU for computations.
image_size = 224: Sets the image resolution to 224x224 pixels, which is a common input size for deep learning models (ResNet).
num_workers = 4: Specifies the number of subprocesses for data loading, improving efficiency.
pin_memory = True: Optimizes GPU performance by keeping data in pinned memory, reducing transfer overhead.
seed = 42: Fixes random seed values for reproducibility across runs.
2. Setting Random Seeds for Reproducibility
To ensure consistent model training, we set random seeds:

torch.manual_seed(Config.seed): Fixes the random seed
np.random.seed(Config.seed)
3. Why Image Size 224?
Many pre-trained models (ResNet, VGG, EfficientNet) are optimized for this input size.
"""

# Configuration class
class Config:
    latent_dim = 256
    batch_size = 32
    epochs = 30
    lr = 1e-4
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    image_size = 224
    num_workers = 4
    pin_memory = True
    seed = 42

# Set random seeds
torch.manual_seed(Config.seed)
np.random.seed(Config.seed)

"""## Data Pipeline Implementation

The ChestXrayDataset class is designed to load and preprocess a dataset containing frontal and lateral X-ray images, along with corresponding captions(findings) and problem categories. The dataset includes multiple preprocessing steps to ensure consistency and usability for deep learning models.

Dual-view image loading is implemented, where each dataset sample includes both a frontal X-ray (anterior-posterior or posterior-anterior) and a lateral X-ray, providing comprehensive diagnostic information.

Label encoding is performed using a label encoder, transforming categorical category labels ("Normal","Pulmonary","Cardiovascular","Musculoskeletal") into numerical indices. These indices are further converted into one-hot encoded vectors to support multi-class classification.

The final dataset output includes:

Frontal image tensor

Lateral image tensor

One-hot encoded label

Associated caption


"""

# Dataset Class
class ChestXrayDataset(Dataset):
    def __init__(self, dataframe, transform=None, label_encoder=None):
        self.df = dataframe.reset_index(drop=True)
        self.transform = transform
        self.label_encoder = label_encoder
        self.classes = label_encoder.classes_ if label_encoder else []

        # Convert labels to indices and store original categories
        if label_encoder is not None and 'categories' in dataframe.columns:
            self.labels = self.label_encoder.transform(dataframe['categories'])
        else:
            self.labels = None

        # Validate image paths
        self._validate_image_paths()

    def _validate_image_paths(self):
        missing = []
        for idx in range(len(self.df)):
            front = self.df.iloc[idx]['front_img']
            lateral = self.df.iloc[idx]['lateral_img']
            if not (os.path.exists(front) and os.path.exists(lateral)):
                missing.append(idx)
        if missing:
            raise FileNotFoundError(f"Missing images at indices: {missing}")

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        try:
            front = Image.open(self.df.iloc[idx]['front_img']).convert('RGB')
            lateral = Image.open(self.df.iloc[idx]['lateral_img']).convert('RGB')

            if self.transform:
                front = self.transform(front)
                lateral = self.transform(lateral)

            # Convert label to one-hot encoding
            label_idx = self.labels[idx]
            one_hot = F.one_hot(torch.tensor(label_idx), num_classes=len(self.classes)).float()

            return {
                'front': front,
                'lateral': lateral,
                'label': one_hot,
                'caption': self.df.iloc[idx]['captions']
            }
        except Exception as e:
            print(f"Error loading sample {idx}: {str(e)}")
            raise

"""## Model Architecture

The DualVAEWithClassifier model is designed for chest X-ray analysis by leveraging a Variational Autoencoder (VAE) combined with a classifier. The model processes dual-view images (frontal and lateral X-rays), encoding them into a shared latent space for both reconstruction and classification tasks.

1. Encoder
Two separate ResNet-18 encoders are used to extract features from frontal and lateral images.
The fully connected layer in each ResNet-18 model is replaced with a linear layer that outputs a latent representation of size Config.latent_dim * 2 (to store both mean and variance).
2. Reparameterization
The model applies the reparameterization trick to generate a latent vector z from the learned mean (mu) and log-variance (logvar).
This allows backpropagation while maintaining stochasticity in the latent space.
3. Decoder
The decoder reconstructs images from the latent representation.
It first transforms the latent vector into a feature map (64 × 56 × 56) and then applies transposed convolution layers to generate a 6-channel output, corresponding to RGB images for both frontal and lateral views.
4. Classifier
The classification module uses a fully connected network that takes the latent representation (mu) as input.
It applies ReLU activation, dropout (0.3), and a final linear layer to predict the diagnostic category.
5. Model Outputs
For each input pair of frontal and lateral images, the model returns:

Reconstructed image (recon) – The autoencoder’s reconstruction of the input.
Latent mean (mu) – The learned mean of the latent space distribution.
Latent log variance (logvar) – The variance of the latent space distribution.
Classification output (cls_out) – The predicted diagnostic category.



Summary Report: DualVAEWithClassifier Model
1. Overview
DualVAEWithClassifier is a dual-encoder variational autoencoder (VAE) with a classifier. It is designed to encode image data using two separate ResNet-based encoders, reconstruct the input through a decoder, and perform classification using a fully connected classifier.

2. Model Components
Front Encoder (front_enc)

Uses a ResNet-based architecture for feature extraction.
Has four residual layers with increasing channel sizes (64 → 128 → 256 → 512).
Uses adaptive average pooling and a fully connected layer (512 output features).
Lateral Encoder (lateral_enc)

Decoder (decoder)
Takes the encoded 512-dimensional feature vector and reconstructs the original input.
Includes fully connected layers, unflattening, and ConvTranspose2D layers for upscaling.
Uses ReLU activations for feature extraction and a Sigmoid activation to normalize outputs.

Classifier (classifier)
Maps the 512-dimensional latent vector to 4 output classes.
Includes fully connected layers, ReLU activation, and Dropout (p=0.3) for regularization.
3. Model Functionality
Dual-Path Encoding: The model extracts features using two separate ResNet-based encoders.
Reconstruction via VAE: The decoder reconstructs the input image from the latent space.
Classification Task: The classifier predicts one of four classes based on the encoded representation.


"""

# Model Architecture
class DualVAEWithClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        # Front view encoder
        self.front_enc = models.resnet18(pretrained=True)
        self.front_enc.fc = nn.Linear(512, Config.latent_dim * 2)

        # Lateral view encoder
        self.lateral_enc = models.resnet18(pretrained=True)
        self.lateral_enc.fc = nn.Linear(512, Config.latent_dim * 2)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(Config.latent_dim * 2, 64 * 56 * 56),
            nn.Unflatten(1, (64, 56, 56)),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 6, 4, 2, 1),
            nn.Sigmoid()
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(Config.latent_dim * 2, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, num_classes)
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, front, lateral):
        # Encode images
        front_mu, front_logvar = torch.chunk(self.front_enc(front), 2, dim=1)
        lateral_mu, lateral_logvar = torch.chunk(self.lateral_enc(lateral), 2, dim=1)

        # Combine latent spaces
        mu = torch.cat([front_mu, lateral_mu], dim=1)
        logvar = torch.cat([front_logvar, lateral_logvar], dim=1)
        z = self.reparameterize(mu, logvar)

        # Decode
        recon = self.decoder(z)

        # Classify
        cls_out = self.classifier(mu)

        return recon, mu, logvar, cls_out
print(DualVAEWithClassifier)

"""## Training Infrastructure

The Trainer class is designed to train and evaluate the DualVAEWithClassifier model efficiently. It manages the training loop, loss calculations, accuracy tracking, and history logging.

1. Model Initialization
The model is moved to the appropriate device (CPU/GPU) for optimized computation.
An Adam optimizer is used with a learning rate of Config.lr to update model parameters.
The best loss is initialized to infinity to track performance improvements.
A history dictionary is created to store training and validation metrics, including loss, reconstruction loss, KL divergence, classification loss, and accuracy.
2. Training Epoch
Each training epoch consists of:

a) Forward Pass

Frontal and lateral X-ray images are retrieved from the batch and transferred to the device.
The model processes both views, generating:
Reconstructed images (recon)
Latent space mean (mu) and variance (logvar)
Classification output (cls_out)

b) Loss Computation
Three loss components are calculated:

Reconstruction Loss (MSE Loss)
Measures how well the VAE reconstructs the input images using Mean Squared Error (MSE).
KL Divergence Loss
Encourages the latent space distribution to be close to a standard normal distribution.
Classification Loss (Binary Cross-Entropy Loss)
Ensures accurate diagnosis prediction from the latent representation.
The total loss is computed as:
Total Loss

Reconstruction Loss
+
KL Loss
+
Classification Loss
Total Loss=Reconstruction Loss+KL Loss+Classification Loss

c) Backpropagation and Optimization

Gradients are computed, and the optimizer updates model parameters based on the total loss.
d) Accuracy Calculation

Predictions (preds) are obtained by selecting the class with the highest probability.
True labels (true_classes) are extracted from the one-hot encoded labels.
Accuracy is computed as the percentage of correct predictions.

3. Metrics Logging
The computed losses and accuracy are stored in the history dictionary for further analysis.
The training progress is displayed, showing epoch number, loss, and accuracy.


"""

# Training and Evaluation Functions
class Trainer:
    def __init__(self, model, train_loader, val_loader, test_loader, num_classes):
        self.model = model.to(Config.device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.num_classes = num_classes
        self.optimizer = optim.Adam(model.parameters(), lr=Config.lr)
        self.best_loss = float('inf')
        self.history = {
            'train': {'loss': [], 'recon': [], 'kl': [], 'cls': [], 'acc': []},
            'val': {'loss': [], 'recon': [], 'kl': [], 'cls': [], 'acc': []}
        }

    def train_epoch(self, epoch):
        self.model.train()
        total_loss = recon_loss = kl_loss = cls_loss = 0
        correct = 0
        total = 0

        for batch in self.train_loader:
            front = batch['front'].to(Config.device)
            lateral = batch['lateral'].to(Config.device)
            labels = batch['label'].to(Config.device)

            self.optimizer.zero_grad()

            recon, mu, logvar, cls_out = self.model(front, lateral)

            # Calculate losses
            batch_recon = F.mse_loss(recon, torch.cat([front, lateral], 1), reduction='sum')
            batch_kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
            batch_cls = F.binary_cross_entropy_with_logits(cls_out, labels)
            loss = batch_recon + batch_kl + batch_cls

            loss.backward()
            self.optimizer.step()

            # Update metrics
            total_loss += loss.item()
            recon_loss += batch_recon.item()
            kl_loss += batch_kl.item()
            cls_loss += batch_cls.item()

            # Calculate accuracy
            preds = torch.argmax(cls_out, 1)
            true_classes = torch.argmax(labels, 1)
            correct += (preds == true_classes).sum().item()
            total += labels.size(0)

        # Store metrics
        self.history['train']['loss'].append(total_loss / len(self.train_loader.dataset))
        self.history['train']['recon'].append(recon_loss / len(self.train_loader.dataset))
        self.history['train']['kl'].append(kl_loss / len(self.train_loader.dataset))
        self.history['train']['cls'].append(cls_loss / len(self.train_loader.dataset))
        self.history['train']['acc'].append(correct / total)

        print(f"Epoch {epoch+1} Train | Loss: {self.history['train']['loss'][-1]:.4f} "
              f"Acc: {self.history['train']['acc'][-1]:.4f}")

    def validate(self, epoch):
        self.model.eval()
        total_loss = recon_loss = kl_loss = cls_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for batch in self.val_loader:
                front = batch['front'].to(Config.device)
                lateral = batch['lateral'].to(Config.device)
                labels = batch['label'].to(Config.device)

                recon, mu, logvar, cls_out = self.model(front, lateral)

                # Calculate losses
                batch_recon = F.mse_loss(recon, torch.cat([front, lateral], 1), reduction='sum')
                batch_kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                batch_cls = F.binary_cross_entropy_with_logits(cls_out, labels)
                loss = batch_recon + batch_kl + batch_cls

                # Update metrics
                total_loss += loss.item()
                recon_loss += batch_recon.item()
                kl_loss += batch_kl.item()
                cls_loss += batch_cls.item()

                # Calculate accuracy
                preds = torch.argmax(cls_out, 1)
                true_classes = torch.argmax(labels, 1)
                correct += (preds == true_classes).sum().item()
                total += labels.size(0)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(true_classes.cpu().numpy())

        # Store metrics
        val_loss = total_loss / len(self.val_loader.dataset)
        self.history['val']['loss'].append(val_loss)
        self.history['val']['recon'].append(recon_loss / len(self.val_loader.dataset))
        self.history['val']['kl'].append(kl_loss / len(self.val_loader.dataset))
        self.history['val']['cls'].append(cls_loss / len(self.val_loader.dataset))
        self.history['val']['acc'].append(correct / total)

        # Save best model
        best_model_path = os.path.join(save_directory, "best_model.pth")

        if val_loss < self.best_loss:
            self.best_loss = val_loss
                 # Save the model state dictionary
            torch.save(self.model.state_dict(), best_model_path)



        print(f"Epoch {epoch+1} Val   | Loss: {val_loss:.4f} "
              f"Acc: {self.history['val']['acc'][-1]:.4f}")

        return all_preds, all_labels

    def evaluate(self):
        self.model.load_state_dict(torch.load(os.path.join(save_directory, "best_model.pth")))


        self.model.eval()
        total_loss = recon_loss = kl_loss = cls_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for batch in self.test_loader:
                front = batch['front'].to(Config.device)
                lateral = batch['lateral'].to(Config.device)
                labels = batch['label'].to(Config.device)

                recon, mu, logvar, cls_out = self.model(front, lateral)

                # Calculate losses
                batch_recon = F.mse_loss(recon, torch.cat([front, lateral], 1), reduction='sum')
                batch_kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                batch_cls = F.binary_cross_entropy_with_logits(cls_out, labels)
                loss = batch_recon + batch_kl + batch_cls

                # Update metrics
                total_loss += loss.item()
                recon_loss += batch_recon.item()
                kl_loss += batch_kl.item()
                cls_loss += batch_cls.item()

                # Calculate accuracy
                preds = torch.argmax(cls_out, 1)
                true_classes = torch.argmax(labels, 1)
                correct += (preds == true_classes).sum().item()
                total += labels.size(0)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(true_classes.cpu().numpy())

        # Calculate final metrics
        test_loss = total_loss / len(self.test_loader.dataset)
        test_acc = correct / total

        print("\nFinal Test Results:")
        print(f"Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}")
        print(classification_report(all_labels, all_preds, target_names=self.model.classes))

        # Plot confusion matrix
        plt.figure(figsize=(10, 8))
        cm = confusion_matrix(all_labels, all_preds)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.model.classes,
                    yticklabels=self.model.classes)
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.show()

    def plot_training_history(self):
        plt.figure(figsize=(12, 8))

        # Loss plot
        plt.subplot(2, 2, 1)
        plt.plot(self.history['train']['loss'], label='Train Loss')
        plt.plot(self.history['val']['loss'], label='Val Loss')
        plt.title('Training and Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()

        # Accuracy plot
        plt.subplot(2, 2, 2)
        plt.plot(self.history['train']['acc'], label='Train Accuracy')
        plt.plot(self.history['val']['acc'], label='Val Accuracy')
        plt.title('Training and Validation Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()

        # Loss components plot
        plt.subplot(2, 2, 3)
        plt.plot(self.history['train']['recon'], label='Recon Loss')
        plt.plot(self.history['train']['kl'], label='KL Loss')
        plt.plot(self.history['train']['cls'], label='Cls Loss')
        plt.title('Training Loss Components')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()

        plt.tight_layout()
        plt.show()

# Data Preparation
# Assuming df is a DataFrame with columns: ['front_img', 'lateral_img', 'categories', 'captions']
# Example data loading:
# df = pd.read_csv('your_dataset.csv')

# Create label encoder
label_encoder = LabelEncoder()
label_encoder.fit(df['categories'])

"""## Data Preparation, Training, and Evaluation Pipeline

This section outlines the data processing, model training, and evaluation pipeline for the DualVAEWithClassifier model. The structured approach ensures efficient data handling, transformation, training, and performance assessment.

1. Data Splitting
To ensure a balanced and representative dataset, the data is split as follows:

70% for training (train_df)

The remaining 30% is split equally for validation and testing (val_df, test_df)
Stratified sampling is applied to maintain class distribution

2. Data Transformations
Image preprocessing is applied to improve model generalization:

Training Transformations:
Resizing to 224×224 pixels
Random horizontal flipping for augmentation
Random rotation (10 degrees) for robustness
Conversion to tensor format
Validation & Test Transformations:
Only resizing and conversion (no augmentation)

3. Dataset and DataLoader Creation
   
The ChestXrayDataset class is used to load the frontal and lateral X-ray images.
Dataloaders ensure efficient batch-wise data processing:
Training Loader: Uses shuffling to prevent overfitting.
Validation & Test Loaders: No shuffling to maintain data order.
Utilizes multiple worker threads (num_workers) for faster loading.

5. Model Initialization and Training Execution
The model is instantiated with the appropriate number of classes.
The Trainer class is initialized with the dataset loaders.
Training loop runs for the specified epochs (Config.epochs), performing:
Training (train_epoch)
Validation (validate)
History tracking for loss and accuracy

7. Performance Evaluation and Visualization
Training history is plotted to analyze loss and accuracy trends.
Final evaluation (evaluate) is conducted on the test dataset.

"""

from torchinfo import summary

# Split data
train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['categories'], random_state=Config.seed)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['categories'], random_state=Config.seed)



# Define transforms
train_transform = transforms.Compose([
    transforms.Resize((Config.image_size, Config.image_size)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
])

val_test_transform = transforms.Compose([
    transforms.Resize((Config.image_size, Config.image_size)),
    transforms.ToTensor(),
])

# Create datasets
train_dataset = ChestXrayDataset(train_df, train_transform, label_encoder)
val_dataset = ChestXrayDataset(val_df, val_test_transform, label_encoder)
test_dataset = ChestXrayDataset(test_df, val_test_transform, label_encoder)

# Create dataloaders
train_loader = DataLoader(
    train_dataset,
    batch_size=Config.batch_size,
    shuffle=True,
    num_workers=Config.num_workers,
    pin_memory=Config.pin_memory
)

val_loader = DataLoader(
    val_dataset,
    batch_size=Config.batch_size,
    shuffle=False,
    num_workers=Config.num_workers,
    pin_memory=Config.pin_memory
)

test_loader = DataLoader(
    test_dataset,
    batch_size=Config.batch_size,
    shuffle=False,
    num_workers=Config.num_workers,
    pin_memory=Config.pin_memory
)

# Main Execution
if __name__ == "__main__":
    # Initialize model and trainer
    model = DualVAEWithClassifier(num_classes=len(label_encoder.classes_))
    print(model)
    model.classes = label_encoder.classes_  # Store class names in model
    trainer = Trainer(model, train_loader, val_loader, test_loader, len(label_encoder.classes_))

    # # Training loop
    for epoch in range(Config.epochs):
        trainer.train_epoch(epoch)
        trainer.validate(epoch)

    # # Plot training history
    trainer.plot_training_history()

    # # Final evaluation
    trainer.evaluate()

"""The **visualize_reconstructions()** function is designed to assess the performance of a DualVAEWithClassifier model by visually comparing the original and reconstructed images. It demonstrates how well the model can reconstruct both front and lateral views of 3D data, such as images from different camera angles.



"""

# Add this after the main execution block
def visualize_reconstructions():
    # Load best model
    model = DualVAEWithClassifier(num_classes=len(label_encoder.classes_))
    model.load_state_dict(torch.load(os.path.join(save_directory, "best_model.pth")))
    model = model.to(Config.device)
    model.eval()

    # Get first 3 test samples
    test_subset = torch.utils.data.Subset(test_dataset, indices=range(3))
    test_loader = DataLoader(test_subset, batch_size=3)
    batch = next(iter(test_loader))

    with torch.no_grad():
        # Move data to device
        front = batch['front'].to(Config.device)
        lateral = batch['lateral'].to(Config.device)

        # Get reconstructions
        recon, _, _, _ = model(front, lateral)

        # Split into front/lateral reconstructions
        front_recon = recon[:, :3, :, :]  # First 3 channels
        lateral_recon = recon[:, 3:, :, :]  # Last 3 channels

    # Plot results
    fig, axs = plt.subplots(3, 4, figsize=(15, 10))

    for i in range(3):
        # Original front
        axs[i, 0].imshow(front[i].cpu().permute(1, 2, 0))
        axs[i, 0].set_title("Original Front")
        axs[i, 0].axis('off')

        # Reconstructed front
        axs[i, 1].imshow(front_recon[i].cpu().permute(1, 2, 0).clip(0, 1))
        axs[i, 1].set_title("Reconstructed Front")
        axs[i, 1].axis('off')

        # Original lateral
        axs[i, 2].imshow(lateral[i].cpu().permute(1, 2, 0))
        axs[i, 2].set_title("Original Lateral")
        axs[i, 2].axis('off')

        # Reconstructed lateral
        axs[i, 3].imshow(lateral_recon[i].cpu().permute(1, 2, 0).clip(0, 1))
        axs[i, 3].set_title("Reconstructed Lateral")
        axs[i, 3].axis('off')

    plt.tight_layout()
    plt.show()

# Add this to the main execution block after evaluation
visualize_reconstructions()

"""## Latent Space Extraction

Summary of extract_latents_and_captions() Function
The function extract_latents_and_captions() is designed to extract the latent vectors (representations) from a trained Variational Autoencoder (VAE) model, along with their corresponding captions and labels from the input data. This function is useful in generating the latent space representations of the data, which can later be used for tasks such as visualization, clustering, or further analysis of the model's understanding.



The model is set to evaluation mode using model.eval(). This ensures that the model's behavior during the inference (i.e., extracting latent representations) does not affect any learned parameters, as dropout layers (if present) are disabled, and batch normalization layers behave differently.
Data Iteration:



The model processes both the front and lateral views of the input data through their respective encoders (front_enc and lateral_enc), each of which outputs a latent vector.
The outputs of the encoders are split into two parts (using torch.chunk()), and the relevant portion (i.e., front_mu and lateral_mu) is extracted.
These two latent representations are concatenated (using torch.cat()) along the appropriate dimension (dim=1) to create a unified latent vector (mu) for each sample. This combined latent vector represents both views of the sample in the model's learned latent space.
Storing Latent Vectors, Captions, and Labels:




"""

# Additional imports
from transformers import GPT2Tokenizer, GPT2Model, AdamW
from skimage.metrics import structural_similarity as ssim
from skimage.metrics import peak_signal_noise_ratio as psnr
# Configuration class
class Config:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    epochs = 30

# 1. Extract Latent Vectors from Trained VAE
def extract_latents_and_captions(dataloader):
    print(dataloader)
    model.eval()
    all_mus = []
    all_captions = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            front = batch['front'].to(Config.device)
            lateral = batch['lateral'].to(Config.device)

            # Get mu from VAE
            front_mu, _ = torch.chunk(model.front_enc(front), 2, dim=1)
            lateral_mu, _ = torch.chunk(model.lateral_enc(lateral), 2, dim=1)
            mu = torch.cat([front_mu, lateral_mu], dim=1)

            all_mus.append(mu.cpu().numpy())
            all_captions.extend(batch['caption'])
            all_labels.extend(torch.argmax(batch['label'], 1).cpu().numpy())

    return np.concatenate(all_mus, axis=0), all_captions, all_labels
    # Extract latent vectors for all datasets
train_mus, train_captions, train_labels = extract_latents_and_captions(train_loader)
val_mus, val_captions, val_labels = extract_latents_and_captions(val_loader)
test_mus, test_captions, test_labels = extract_latents_and_captions(test_loader)
# Print the total length of each dataset
print(f"Total training samples: {len(train_mus)}")
print(f"Total validation samples: {len(val_mus)}")
print(f"Total test samples: {len(test_mus)}")

# Display the first 5 rows of each dataset
# Create dataframes for each dataset
train_df = pd.DataFrame({'Latent Vectors': list(train_mus), 'Captions': train_captions, 'Labels': train_labels})
val_df = pd.DataFrame({'Latent Vectors': list(val_mus), 'Captions': val_captions, 'Labels': val_labels})
test_df = pd.DataFrame({'Latent Vectors': list(test_mus), 'Captions': test_captions, 'Labels': test_labels})

# Display the first 5 rows for each dataset
print("\nTraining Dataset (First 5 rows):")
print(train_df.head())

print("\nValidation Dataset (First 5 rows):")
print(val_df.head())

print("\nTest Dataset (First 5 rows):")
print(test_df.head())

"""## Text-Latent Dataset Preparation

This report summarizes the implementation of a Text-to-Latent Dataset, which maps textual descriptions (captions) to latent vector representations extracted from a trained Variational Autoencoder (VAE). The dataset is designed to facilitate tasks such as text-to-image retrieval and text-conditioned generation by enabling models to learn a joint representation of textual and visual features.


"""

# 2. Text-to-Latent Dataset
class TextLatentDataset(Dataset):
    def __init__(self, mus, captions, tokenizer, max_length=128):
        self.mus = mus
        self.captions = captions
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.captions)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.captions[idx],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'mu': torch.tensor(self.mus[idx], dtype=torch.float)
        }

# Initialize tokenizer and datasets
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

train_text_dataset = TextLatentDataset(train_mus, train_captions, tokenizer)
for i in range(5):
    print(train_text_dataset[i])
val_text_dataset = TextLatentDataset(val_mus, val_captions, tokenizer)
test_text_dataset = TextLatentDataset(test_mus, test_captions, tokenizer)

# Create dataloaders
text_train_loader = DataLoader(train_text_dataset, batch_size=32, shuffle=True)
text_val_loader = DataLoader(val_text_dataset, batch_size=32, shuffle=False)
text_test_loader = DataLoader(test_text_dataset, batch_size=32, shuffle=False)

"""## Text-to-Latent Model Architecture


The Text-to-Latent Model that converts textual descriptions into a latent space representation. The model utilizes GPT-2 as a feature extractor and a regression layer to map the extracted features into a 512-dimensional latent space.

Methodology
Model Architecture
A pretrained GPT-2 model processes input text.
A linear regression layer maps the final hidden state of GPT-2 to the latent space.
Training Process
Dataset: Text samples paired with their corresponding latent representations.
Loss Function: Mean Squared Error (MSE) to measure the difference between predicted and actual latent representations.
Optimizer: AdamW with a learning rate of 1e-4.
The model is trained and validated using separate datasets.
Validation & Performance Tracking
Training and validation losses are recorded after each epoch.
A learning curve is plotted to analyze model performance over time.
Model Saving & Loading
The trained model is saved as text_model.pth.
The saved model is reloaded and verified for integrity.
Results & Insights
The model successfully learned to map text inputs into a structured latent space.
Training and validation losses indicate stable learning behavior.
The trained model is stored and ready for further integration into a multi-modal AI system.
Conclusion
The Text-to-Latent Model effectively bridges the gap between textual and latent representations, providing a robust foundation for text-based feature extraction in machine learning applications. Future improvements may involve fine-tuning GPT-2 for domain-specific text and exploring different latent space dimensions for optimization.

"""

# 3. Text-to-Latent Model
class TextToLatent(nn.Module):
    def __init__(self, latent_dim=512):
        super().__init__()
        self.gpt2 = GPT2Model.from_pretrained('gpt2')
        self.regressor = nn.Linear(self.gpt2.config.hidden_size, latent_dim)

    def forward(self, input_ids, attention_mask):
        outputs = self.gpt2(input_ids, attention_mask=attention_mask)
        last_hidden = outputs.last_hidden_state[:, -1, :]  # Use last token
        return self.regressor(last_hidden)

text_model = TextToLatent().to(Config.device)
optimizer = AdamW(text_model.parameters(), lr=1e-4)
criterion = nn.MSELoss()

# 4. Train Text Model
text_trainer_history = {'train_loss': [], 'val_loss': []}

for epoch in range(30):  # Adjust epochs
    # Training
    text_model.train()
    train_loss = 0
    for batch in text_train_loader:
        input_ids = batch['input_ids'].to(Config.device)
        attention_mask = batch['attention_mask'].to(Config.device)
        mu = batch['mu'].to(Config.device)

        optimizer.zero_grad()
        pred_mu = text_model(input_ids, attention_mask)
        loss = criterion(pred_mu, mu)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    # Validation
    text_model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch in text_val_loader:
            input_ids = batch['input_ids'].to(Config.device)
            attention_mask = batch['attention_mask'].to(Config.device)
            mu = batch['mu'].to(Config.device)

            pred_mu = text_model(input_ids, attention_mask)
            val_loss += criterion(pred_mu, mu).item()

    # Save history
    text_trainer_history['train_loss'].append(train_loss/len(text_train_loader))
    text_trainer_history['val_loss'].append(val_loss/len(text_val_loader))
    print(f"Epoch {epoch+1}: Train Loss: {text_trainer_history['train_loss'][-1]:.4f}, Val Loss: {text_trainer_history['val_loss'][-1]:.4f}")

    # Save text model
    print(save_directory)
    # Save text model
    model_path = os.path.join(save_directory, "text_model.pth")
    torch.save(text_model.state_dict(), model_path)

    # Verify model is saved
    if os.path.exists(model_path):
        print(f"✅ Model successfully saved at {model_path}")
    else:
        print(f"❌ Model was NOT saved! Check for errors.")

    # Try loading the saved model
    if os.path.exists(model_path):
        text_model.load_state_dict(torch.load(model_path))
        print(f"✅ Model successfully loaded from {model_path}")
    else:
        raise FileNotFoundError(f"❌ Model file NOT found: {model_path}")
    torch.save(text_model.state_dict(), os.path.join(save_directory, "text_model.pth"))

    # Plot training curves
    plt.plot(text_trainer_history['train_loss'], label='Train Loss')
    plt.plot(text_trainer_history['val_loss'], label='Val Loss')
    plt.title('Text Model Training History')
    plt.legend()
    plt.show()

"""## Image Generation Pipeline And Synthetic Image Evaluation

1. Image Generation Process
The generate_images function converts text(findings) into synthetic images using a Text-to-Latent Model and a Variational Autoencoder (VAE):

Captions are tokenized and processed through the text model, generating a latent representation.
The VAE decoder reconstructs images from the latent space.
The generated images are split into front and lateral views for further evaluation.
2. Evaluation Metrics for Synthetic Images
The evaluate_synthetic_images function measures the quality of generated images using:

Structural Similarity Index (SSIM) – Evaluates how structurally similar generated images are to real ones.
Peak Signal-to-Noise Ratio (PSNR) – Measures image quality by comparing pixel values.
Classification Accuracy – Assesses the generated images by passing them through the classification model.
3. Visualization of Generated Images
The plot_sample_generated function generates sample images from text and displays:

Front and lateral views of the generated images.
The corresponding caption used for generation.
4. Results & Insights
The SSIM, PSNR, and classification accuracy provide objective metrics for evaluating image quality.


"""

# 1. Image Generation Function
def generate_images(text_model, vae, tokenizer, caption, device):
    text_model.eval()
    vae.eval()

    # Tokenize caption
    inputs = tokenizer(
        caption,
        return_tensors='pt',
        max_length=128,
        padding='max_length',
        truncation=True
    ).to(device)

    # Predict latent
    with torch.no_grad():
        pred_mu = text_model(inputs['input_ids'], inputs['attention_mask'])

    # Decode images
    with torch.no_grad():
        recon = vae.decoder(pred_mu)
        print(vae.decoder)
    # Split into front and lateral
    front = recon[:, :3, :, :]
    lateral = recon[:, 3:, :, :]

    return front, lateral

# 2. Evaluation Metrics
def evaluate_synthetic_images(test_loader):
    text_model.load_state_dict(torch.load(os.path.join(save_directory, "text_model.pth")))

    ssim_scores = []
    psnr_scores = []
    accuracies = []

    with torch.no_grad():
        for batch in test_loader:
            real_front = batch['front'].to(Config.device)
            real_lateral = batch['lateral'].to(Config.device)
            captions = batch['caption']

            # Generate images
            gen_front, gen_lateral = generate_images(
                text_model, model, tokenizer, captions, Config.device
            )

            # Resize images if they are too small
            if real_front.shape[2] < 7 or real_front.shape[3] < 7:
                real_front = F.interpolate(real_front, size=(7, 7), mode='bilinear', align_corners=False)
                gen_front = F.interpolate(gen_front, size=(7, 7), mode='bilinear', align_corners=False)

            # Calculate metrics per image
            for i in range(real_front.size(0)):
                # SSIM
                ssim_front = ssim(
                    real_front[i].cpu().numpy(),
                    gen_front[i].cpu().numpy(),
                    data_range=1.0,
                    win_size=3,  # Adjust based on image size
                    channel_axis=-1  # Ensure for RGB images
                )

                # PSNR
                psnr_front = psnr(
                    real_front[i].cpu().numpy(),
                    gen_front[i].cpu().numpy(),
                    data_range=1.0
                )

                ssim_scores.append(ssim_front)
                psnr_scores.append(psnr_front)

            # Classification accuracy
            _, _, _, cls_out = model(gen_front, gen_lateral)
            preds = torch.argmax(cls_out, 1)
            true = torch.argmax(batch['label'], 1).to(Config.device)
            accuracies.extend((preds == true).cpu().numpy())

    print(f"SSIM: {np.mean(ssim_scores):.4f}")
    print(f"PSNR: {np.mean(psnr_scores):.4f}")
    print(f"Classification Accuracy: {np.mean(accuracies):.4f}")


# 3. Generate Sample Images
def plot_sample_generated():
    sample = next(iter(test_loader))
    caption = sample['caption'][1]

    # Generate
    gen_front, gen_lateral = generate_images(
        text_model, model, tokenizer, [caption], Config.device
    )

    # Plot
    fig, ax = plt.subplots(1, 2)
    ax[0].imshow(gen_front[0].permute(1,2,0).cpu().numpy().clip(0,1))
    ax[0].set_title("Generated Front")
    ax[1].imshow(gen_lateral[0].permute(1,2,0).cpu().numpy().clip(0,1))
    ax[1].set_title("Generated Lateral")
    plt.suptitle(f"Caption: {caption}")
    plt.show()

# Execute evaluation
evaluate_synthetic_images(test_loader)
plot_sample_generated()

print(text_model)